---
title: "03. Контрастивная симуляция. Упражнения"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

3.1. Используйте частотное распределение лемм ДетКорпуса, чтобы
сгенерировать два искусственных случайных корпуса (c1 и c2) размером
10 тыс. слов каждый. Используемые при генерации вероятности слов в
обоих корпусах должны совпадать. Проанализируйте, каких значений
достигает G2 для слов разного уровня частотности. Постройте график,
визаулизирующий соотношение G2 и суммарной частотности слова в двух
корпусах.

3.2. В проведенном в скрипте 03contrastice-simulations.Rmd
эксперименте двухкратное увеличение вероятности достаточно редких слов
на букву ц- оказалось недостаточно сильным эффектом, чтобы его можно
было уверенно зафиксировать с помощью G2. Проверьте, будут ли более
сильные эффекты (различия в вероятности слов) распознаваться надежнее.
Сгенерируйте два лексических распределения (на основе вероятностей
ДетКорпуса) по 100 тыс. слов каждое таким образом, чтобы слова на
букву ц- во втором из них были в 10 раз более вероятными, чем в
первом. Как изменяется соотношение в значении G2 слов на букву ц- и
всех прочих? Какие минимальные значения суммарных частотностей слова и
пороговые значения G2 следует выбрать, чтобы уверенно идентифицировать 
эффекты такого масштаба?

3.3. Сгенерируйте два лексических распределения по 100 тыс. слов таким
образом, чтобы во втором из них была вдвое повышена вероятность слов
на букву м-, длина которых не менее четырех букв. Среди таких слов
есть значительно более частотные, чем слова на букву
ц-. Проанализируйте соотношение значения G2 и частотности для 10-20
самых частотных слов на м-. Можно ли полагаться на G2 для определения
двухкратного увеличения вероятности слов такого уровня частотности?

3.4. Повторите эксперимент со словами на букву м-, но используйте в
качестве меры контрастного использования простое отношение
нормализованных частотностей по методу simple maths Адама Килгарифа. 
Удается ли в этом случае подобрать минимальные пороги суммарной
частотности и отношения частотностей, чтобы надежно выделять слова с
неодинаковой вероятностью в двух корпусах?

3.5. Как изменятся результаты эксперимента из задания 3.4, если до
расчета отношений удалить стоп-слова в обоих корпусах? Проведите
эксперимент. 


