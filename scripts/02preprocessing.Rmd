---
title: "Препроцессинг"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Варианты токенизации: N-граммы

Пример токенизации данных с дефолтными настройками и с измененными
(биграммы). 

Функция unnest_tokens() позволяет работать не только со словами, но и,
напрмиер, с n-граммами, то есть сочетаниями из n слов. Важно понимать,
что n-граммы образуются “внахлёст”:

```{r}
library(tidytext)
library(dplyr)
txt <- tibble(text = "А ловко ты это придумал, в начале я даже не понял, слушай молодец")
txt %>% unnest_tokens(bigram, text, token = "ngrams", n = 3)
```

Далее мы будем использовать датасет с стихотворениями поэтов
Серебряного века.  Немного углубимся в создание и визуализацию
биграмм.

Загружаем датасет. Обратите внимание, что файл датасета, созданный в
ОС Windows, содержит текст не в Unicode (кодировка UTF-8), а в
устаревшей восьмибитной кодировке CP-1251. Поэтому при загрузке на сервере
необходимо перекодировать данные (см. параметр locale при вызове
функции read_csv ниже).

```{r}
library(readr)
silver_poems <- read_csv("~/cmta2021/data/silver_poems.csv", locale = readr::locale(encoding = "cp1251"))
```

Извлекаем биграммы:

```{r}
bigrams <- silver_poems %>% 
  unnest_tokens(bigram, review, token = "ngrams", n = 2)
```

Строим частотный список биграмм: 

```{r}
bigrams.freq <- bigrams %>% count(bigram, sort=TRUE)
bigrams.freq
```

Следующая задача — получить частотный список только тех биграмм,
которые не содержат стоп-слов (чтобы увидеть более содержательные
биграммы). Для этого потребуется выполнить несколько шагов. 

Разбиваем колонку с биграммами на две колонки с отдельными словами,
составляющими биграмму (функция tidyr::separate):

```{r}
library(tidyr)
bigrams.sep <- bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") 
```

Загружаем список стоп-слов для русского языка из пакета stopwords и
оформляем его в виде колонки таблицы. Не забудьте просмотреть список
полностью перед тем, как воспользоваться им — там есть сюрпризы!

```{r}
library("stopwords")
rustop <- tibble(word = stopwords("ru"))
View(rustop)
```

Исключаем стоп-слова (на первой и на второй позиции в биграмме):

```{r}
bigrams.nonstop <- bigrams.sep %>% 
  anti_join(rustop, by = c(word1="word")) %>% 
  anti_join(rustop, by = c(word2="word"))
```

Строим частотный список:

```{r}
bigrams.nonstop %>% 
  count(word1, word2, sort=TRUE)
```

## Лемматизация

Лемматизация — приведение словоформ в тексте к начальным формам
(леммам). Позволяет абстрагироваться от словоизменения (падежных форм
существительных и прилагательных, временных форм глаголов и т.п.) и
сосредоточиться на лексических единицах текста.

Для лемматизации мы будем использовать mystem — свободно
распространяемую утилиту, разработанную в Яндексе. Mystem работает по
принципу фильтра: принимает на вход текст и возвращает текст в
лемматизированной форме и с грамматической информацией (в зависимости
от опций вызова). Вызвать утилиту можно из командной строки
операционной системы или непосредственно из R. 

Mystem уже установлен на сервере, но если вам потребуется, то нужно
будет установить его на компьютер.

Как скачать mystem?
1. https://yandex.ru/dev/mystem/
2. Скачиваем версию 3.1 для вашего устройства
3. После загрузки, переместите mystem в документы или туда, откуда вам удобнее будем его вызывать.

Вызовем mystem с помощью функции system2 (текст для лемматизации — в
колонке text, обязательно должна присутствовать опция stdout =
TRUE). Результат лемматизации сохраним в колонке lem.

```{r}
silver_poems$lem <- system2("mystem", c("-d", "-l", "-c"), input = silver_poems$text, stdout = TRUE)
```

Теперь мы можем построить лемматизированный частотный список. Обратите
внимание, что в качестве колонки с исходным текстом мы берем теперь не
text, а lem, где находятся уже лемматизированные данные.

```{r}
sp.lemmas <- silver_poems %>% unnest_tokens(word, lem)
```

**NB** Правильная последовательность операций: СНАЧАЛА лемматизация,
ПОТОМ токенизация. Поскольку в этом случае есть возможность применить
контекстое снятие омонимии, встроенное в mystem (опция "-d").

Вот у нас имеется готовая колонка с леммами. Можно строить частотный
список: 

```{r}
sp.freq <- sp.lemmas %>% count(word, sort=TRUE)
sp.freq
```

### Mystem: извлечение грамматической информации

Что еще можно сделать с помощью mystem?  Можно заглянуть в
документацию https://yandex.ru/dev/mystem/doc/ и извлечь не только
леммы, но и грамматическую информацию о слове (например части речи,
род). 

Для вывода грамматической информации добавим опции "-ig" в вызов
mystem и сохраним результат в колонке gram.

```{r}
silver_poems$gram <- system2("mystem", c("-d", "-l", "-c", "-ig"), input = silver_poems$text, stdout = TRUE)
```

В грамматической информации для нас могут быть интересны части речи и
другие граммемы: S - существительное, V — глагол, APRO -
местоимение-прилагательное и т.п. Расшифровку всех обозначений граммем
можно найти в документации mystem.

### Регулярные выражения

Что это? Это специальный язык для описания шаблонов строк, который
используется для поиска определенных строк, проверки их на
соответствие какому-либо шаблону и другой подобной работы. Регулярные
выражения — незаменимый инструмент для парсинга текстовых данных,
чистки текста в ходе препроцессинга и извлечения определенной
информации из текста.

Полезные ресурсы:
1. На английском https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/regex
2. На русском, структурированно и понятно http://website-lab.ru/article/regexp/shpargalka_po_regulyarnyim_vyirajeniyam/

Пример использования при чистке текста:

Избавляемся от пунктуации в текстах (используем функцию gsub из
базового R):

```{r}
silver_poems$clean <- gsub("[[:punct:]]", " ", silver_poems$text) #избавляемся от пунктуации
```

Избавляемся от цифр (используем функцию str_replace_all, функционально
аналогичную gsub, но из пакета stringr):

```{r}
library(stringr)
silver_poems$clean <- str_replace_all(silver_poems$clean, "[0-9]+", " ")
```

### Извлечение лемм и частей речи из вывода mystem

Здесь мы извлечем в отдельную колонку части речи. А также избавимся от
всего лишнего в колонке lem. Для этого воспользуемся возможностью
передать в качестве токенизатора в unnest_tokens любую функцию. В
нашем случае это будет функция поиска по регулярному выражению
str_extract_all из пакета stringr.

Легко заметить, что в колонке gram части речи указаны после символа =
заглавными латинскими буквами. Извлечем леммы вместе с частью
речи. Для этого в параметре pattern опишем структуру токенов
(лемма=ЧАСТЬ_РЕЧИ) с помощью регулярного выражения. Все, что не
совпадает с этим шаблоном, будет просто проигнорировано.

```{r}
sp.gram <- silver_poems %>% unnest_tokens(word, gram, token = stringr::str_extract_all, pattern="\\w+=[A-Z]+")
```

Разберем состав регулярного выражения: 

* \\w → любая буква
* + → одно или более повторений предыдущего символа
* \\w+ → любая буква один или более раз
* = → просто символ “=”
* [A-Z] → любой символ из диапазона A-Z (любая заглавная латинская
  буква)
* [A-Z]+ → одна или более заглавных латинских букв
* "\\w+=[A-Z]+" → одна или более любых букв, за которыми следует
  символ “равно”, за которым следует одна или более заглавных
  латинских букв.
  
Теперь осталось только отделить части речи от лемм с помощью уже
известной нам функции tidyr::separate по символу "=".

```{r}
sp.lempos <- sp.gram %>%
    separate(word, c("lemma", "pos"), sep = "=") 
```

P.S. Для лемматизации и для определения частей речи можно использовать
пакет udpipe. Его преимущества: не требуется пост-обработка результата
лемматизации регулярными выражениями (получается сразу удобный
токенизированный дата-фрейм) с леммами, частями речи, синтаксической
информацией. Его недостатки: более медленная обработка, больше ошибок
при определении частей речи и грамматических форм слов.

Если интересно, ассистенты могут подготовить дополнительную информацию
по udpipe и примеры использования.


