\documentclass[10pt,svgnames]{beamer}
\graphicspath{{images/}}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{booktabs}
\usepackage{biblatex}

\usetheme[titleformat=smallcaps,
numbering=fraction,
subsectionpage=progressbar,
progressbar=frametitle]{metropolis}
\usecolortheme[light,accent=orange]{solarized}
\setbeamercovered{transparent}
\newcommand{\dir}{\text{Dirichlet}}
\newcommand{\mult}{\text{Multinomial}}
\usetikzlibrary{arrows,decorations.pathmorphing,fit,positioning,automata}
\addbibresource{local.bib}



\title[QTA 09] % (optional, use only with long paper }
{Языковые модели}

\subtitle
{Квантитативный анализ текста} % (optional)

\author%[Author, Another] % (optional, use only with lots of authors)
{Кирилл Александрович Маслинский}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute%[Universities of Somewhere and Elsewhere] % (optional, but mostly needed)
{ЕУСПб}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date%[Short Occasion] % (optional)
{16.05.2022 / 09}

\subject{natural language processing, text mining}
% This is only inserted into the PDF information catalog. Can be left
% out. 



% If you have a file called "university-logo-filename.xxx", where xxx
% is a graphic format that can be processed by latex or pdflatex,
% resp., then you can add a logo as follows:

% \pgfdeclareimage[height=0.5cm]{university-logo}{university-logo-filename}
% \logo{\pgfuseimage{university-logo}}

% Delete this, if you do not want the table of contents to pop up at
% the beginning of each subsection:

\newcommand{\plate}[1]{\begingroup\setbeamercolor{background canvas}{bg=Beige}
  % \begin{frame}<beamer>{Outline}
  %   \tableofcontents[sectionstyle=show/hide,subsectionstyle=show/shaded/hide]
  % \end{frame}
  \begin{frame}[plain]
  \vfill
  \centering
  \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
    \usebeamerfont{title}#1\par%
  \end{beamercolorbox}
  \vfill
  \end{frame}
  \endgroup
}

% \AtBeginSection[]
% {
%   \begin{frame}<beamer>[plain]{План}
%     \tableofcontents[sectionstyle=shaded,subsectionstyle=hide]
%   \end{frame}
% }

% \AtBeginSubsection[]
% {
%   \begin{frame}<beamer>[plain]{План}
%     \tableofcontents[sectionstyle=shaded,subsectionstyle=show]
%   \end{frame}
% }

\newcommand{\tb}[1]{\colorbox{yellow}{#1}\space}
\newcommand{\Sp}[1]{\colorbox{green}{#1}\space}
\newcommand{\Sn}[1]{\colorbox{red}{#1}\space}


\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Контекст}


\subsection{Модель контекста: N-граммы}

\begin{frame}

  \frametitle{N-граммы}
  
  N последовательно стоящих друг за другом слов.
  
  \begin{block}{Униграммы}
    \alert<2>{Восторг} \alert<3>{внезапный} \alert<4>{ум}
    \alert<5>{пленил} .
  \end{block}

  \begin{block}{Биграммы}
  \alert<2>{Восторг} \alert<2-3>{внезапный} \alert<3-4>{ум}
  \alert<4-5>{пленил} \alert<5>{<.>}    
  \end{block}

  \begin{block}{Триграммы}
  \alert<2>{<s>}  \alert<2-3>{Восторг} \alert<2-4>{внезапный}
  \alert<3-5>{ум} \alert<4-5>{пленил} \alert<5>{<.>}
  \end{block}
\end{frame}

\section{Вероятность языковых событий}

\begin{frame}
  \frametitle{Предсказание слова}
  искусственный \only<1>{\alert{...?}}\only<2>{\alert{интеллект?}

    \href{http://detcorpus.ru/search/\#concordance?corpname=detcorpus&tab=advanced&queryselector=cql&structs=s\%2Cg&refs=\%3Ddoc.id\%2C\%3Ddoc.text_year\%2C\%3Df.id&default_attr=lemma&cql=\%5Blemma\%3D\%22\%D0\%B8\%D1\%81\%D0\%BA\%D1\%83\%D1\%81\%D1\%81\%D1\%82\%D0\%B2\%D0\%B5\%D0\%BD\%D0\%BD\%D1\%8B\%D0\%B9\%22\%5D\%20\%5Btag\%3D\%22S\%22\%5D&selection=\%7B\%7D&showresults=1&f_freqml=\%5B\%7B\%22attr\%22\%3A\%22lemma\%22\%2C\%22base\%22\%3A\%22kwic\%22\%2C\%22ctx\%22\%3A0\%7D\%5D&operations=\%5B\%7B\%22name\%22\%3A\%22cql\%22\%2C\%22arg\%22\%3A\%22\%5Blemma\%3D\%5C\%22\%D0\%B8\%D1\%81\%D0\%BA\%D1\%83\%D1\%81\%D1\%81\%D1\%82\%D0\%B2\%D0\%B5\%D0\%BD\%D0\%BD\%D1\%8B\%D0\%B9\%5C\%22\%5D\%20\%5Btag\%3D\%5C\%22S\%5C\%22\%5D\%22\%2C\%22active\%22\%3Atrue\%2C\%22query\%22\%3A\%7B\%22queryselector\%22\%3A\%22cqlrow\%22\%2C\%22cql\%22\%3A\%22\%5Blemma\%3D\%5C\%22\%D0\%B8\%D1\%81\%D0\%BA\%D1\%83\%D1\%81\%D1\%81\%D1\%82\%D0\%B2\%D0\%B5\%D0\%BD\%D0\%BD\%D1\%8B\%D0\%B9\%5C\%22\%5D\%20\%5Btag\%3D\%5C\%22S\%5C\%22\%5D\%22\%2C\%22default_attr\%22\%3A\%22lemma\%22\%7D\%2C\%22id\%22\%3A7499\%7D\%5D&results_screen=frequency}{а в корпусе русской детской литературы XX—XXI вв.?}
  }
\end{frame}

\begin{frame}
  \frametitle{Вероятность языковых событий}
  \begin{itemize}
  \item Классическая вероятность: монетка или кость
    \pause
  \item Вероятность, основанная на частотности (урна с шарами, мешок слов)
  \item В лингвистике считаем события в \structure{корпусе}
  \item вероятность = относительная частотность
  \end{itemize}
  \begin{block}{Пример расчета вероятности слова}
    Всего слов в корпусе = 46,804,371
    
    искусственный = 121
    
    $$
    P({\text{искусственный}}) = 
    \frac{121}{46804371} \approx 0.000002585 = 2.585\ \text{IPM}
    $$
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Вероятность пары слов}
  \LARGE
  \begin{itemize}
  \item $P({\text{искусственный}}) = \frac{121}{46804371}$
  \item $P({\text{интеллект}}) = \frac{179}{46804371}$
  \item $P({\text{искусственный интеллект}}) = \alert{?}$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Совместная вероятность}
  \large
  \begin{equation}
    P(A \land B) = P(A) \cdot P(B)\quad |\quad \text{IFF A и B независимы}
  \end{equation}

  \only<2>{
    $$P({\text{искусственный интеллект}}) =
    \frac{121}{46804371}\cdot\frac{179}{46804371} =
    $$
    $$
    = 0.00000000000989 =
    0.00000989\ \text{IPM}$$
  }
\end{frame}

\begin{frame}
  \frametitle{Условная вероятность}
  \begin{equation}
    P(B|A) = \frac{P(B \land A)}{P(A)}
  \end{equation}

  $$
    P(\text{интеллект}|\text{искусственный}) =
    \frac{P(\text{искусственный интеллект})}{P(\text{искусственный})}
    =$$

    $$
    = \frac{\frac{4}{46804371}}{\frac{121}{46804371}} = \frac{4}{121} = 0.033$$
\end{frame}

\begin{frame}
  \frametitle{Предсказание слова в контексте}
  \begin{block}{}
    сильный искусственный \only<1>{\alert{...?}}\only<2>{\alert{интеллект!}}
  \end{block}
  \only<3>{
    $$
    P(\text{интеллект}|\text{сильный искусственный}) = 
    $$

    $$
    \frac{f(\text{сильный искусственный
        интеллект})}{f(\text{сильный искусственный})} = \frac{?}{?} = ?
    $$}
\end{frame}

\subsection{Коллокации}
\begin{frame}
  \frametitle{Модель для коллокаций}
  Слова, встречающиеся в текстах рядом чаще, чем можно ожидать в
  результате случайности.

  \begin{tabular}[c]{r|c|c}
    & интеллект & $\neg$ интеллект \\
    \hline
    искусственный & \\
    \hline
    $\neg$ искусственный & \\
  \end{tabular}
\end{frame}

\subsection{Языковая модель}

\begin{frame}
  \frametitle{Вероятность предложения}
  \LARGE
  \begin{itemize}
  \item P(Создан сильный искусственный интеллект.) = ?
  \item P(Создан сильный искусственный крокодил.) = ?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Language model}
  Языковая модель — приписывает вероятность фрагменту текста
  (высказыванию, предложению...)
  
  В хорошей модели вероятности языковых фрагментов соответствуют их
  относительной частотности в текстах.

  Иными словами: 
  \begin{itemize}
  \item максимизирует вероятность реальных текстов
  \item минимизирует вероятность нереальных текстов
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Униграммная языковая модель}
  \begin{block}{}
    Создан\alert{$^{31}$} сильный\alert{$^{1905}$} искусственный\alert{$^{121}$} интеллект\alert{$^{71}$}!
  \end{block}
  \begin{block}{}
    Создан\alert{$^{31}$} сильный\alert{$^{1905}$} искусственный\alert{$^{121}$} крокодил\alert{$^{442}$}!
  \end{block}

  \only<2>{

  $$
  \frac{31}{46804371} \cdot \frac{1905}{46804371} \cdot \frac{121}{46804371}
  \cdot \frac{71}{46804371} = 1.06 \times 10^{-22}
  $$

    $$
  \frac{31}{46804371} \cdot \frac{1905}{46804371} \cdot \frac{121}{46804371}
  \cdot \frac{442}{46804371} = 6.58 \times 10^{-22}
  $$
  
  \alert{Крокодил в шесть раз вероятнее!}
}

\end{frame}

\begin{frame}
  \frametitle{Chain rule of probability}
  \begin{equation}
    P(A,B)=P(B|A)P(A)
  \end{equation}

  $$
    P(W_1,W_2,W_3,W_4) = 
    $$
    $$
    P(W_4|W_1,W_2,W_3)  \cdot P(W_3|W_1,W_2)
    \cdot P(W_2|W_1) \cdot P(W_1)
    $$

    P(Создан сильный искусственный крокодил) =
    P(Создан) P(сильный|Создан)
    P(искусственный|Создан сильный)
    P(крокодил|Создан сильный искусственный)
\end{frame}

\begin{frame}
  \frametitle{Цепь Маркова}
  \begin{itemize}
  \item система с конечным числом состояний
  \item следующее состояние зависит только от текущего
  \end{itemize}
  Применительно к тексту: 

  \structure{Следующее слово зависит только от предыдущего (N предыдущих)}
\end{frame}


\begin{frame}
  \frametitle{Биграммная языковая модель}
  \structure{Markov assumption}:
  $$
  P(\text{Создан сильный искусственный интеллект}) \approx
  $$

  $$
  P(\text{интеллект}|\text{искусственный}) \cdot
  P(\text{искусственный}|\text{сильный})
  $$

  $$
  \cdot P(\text{сильный}|\text{создан}) =
  $$

  $$
  \frac{2.21}{45.65} \cdot \frac{0.01}{189.7} \cdot \frac{\alert{0.003}}{16.28} \approx
  4.7 \times 10^{-10}
  $$
\end{frame}

\begin{frame}
  \frametitle{Биграммная языковая модель}
  \structure{Markov assumption}:
  $$
  P(\text{Создан сильный искусственный крокодил}) \approx
  $$

  $$
  P(\text{крокодил}|\text{искусственный}) \cdot
  P(\text{искусственный}|\text{сильный})
  $$

  $$
  \cdot P(\text{сильный}|\text{создан}) =
  $$

  $$
  \frac{\alert{\only<1>{0}\only<2>{0.001}}}{45.65} \cdot \frac{0.01}{189.7} \cdot \frac{0.003}{16.28} \approx
 \only<1>{\alert{0}} \only<2>{2.13 \times 10^{-13}}
  $$
\end{frame}

\begin{frame}
  \frametitle{Проблема редких слов}
  Проблема недостаточных данных (\structure{sparse data})
  \begin{itemize}
  \item В результате обучения на конечном корпусе очень многие
    N-граммы получат нулевую вероятность
  \item Хотя в действительности должны иметь ненулевую (встречаются в
    текстах)
  \end{itemize}
  Если в предложении встречается слово, отсутствовавшее в корпусе,
  вероятность такого предложения в модели \alert{равна нулю}!

  \begin{block}{Задача}
    Добиться, чтобы модель приписывала ненулевую вероятность любому тексту.
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Сглаживание Лапласа}
  \framesubtitle{Add-one smoothing}
  \structure{Добавить 1 ко всем частотам в корпусе}
  
  Для униграммной модели (сглаженная относительная частотность слова):
\begin{equation}
    P_{Laplace}=\frac{f + 1}{N + V}
  \end{equation}  
\end{frame}

\begin{frame}
  \frametitle{Другие методы сглаживания}
  \begin{block}{Идея}
    Оценить вероятность никогда не встречавшихся N-грамм на основании
    вероятности N-грамм, встречавшихся один раз
  \end{block}
  \begin{itemize}
  \item Good-Turing discounting
  \item Kneser-Ney smoothing
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Биграммная языковая модель}
  \structure{Markov assumption}:
  P(\text{Создан сильный искусственный крокодил}) $\approx$
  P(\text{крокодил}|\text{искусственный}) 
  P(\text{искусственный}|\text{сильный})
  P(\text{сильный}|\text{создан})

  \begin{columns}
    \column{.7\textwidth}
    $$
    \frac{0.001}{45.65} \cdot
    \frac{0.01}{189.7} \cdot \frac{0.003}{16.28} \approx 2.13 \times
    10^{-13}
  $$

  $$
  \frac{\alert{0.001}}{45.2} \cdot \frac{0.01}{187.8} \cdot
  \frac{0.004}{16.12} \approx 2.92 \times 10^{-13}
  $$
  \column{.3\textwidth}
  \alert{Интеллект}:
  
  $$
  4.7 \times 10^{-10}$$
  
  $$6.4 \times 10^{-10}$$
\end{columns}
\end{frame}


\begin{frame}
  \frametitle{Триграммная языковая модель}
  $$
  P(\text{Создан сильный искусственный интеллект}) \approx
  $$

  $$
  P(\text{интеллект}|\text{сильный искусственный}) \cdot
  $$
  $$
  P(\text{искусственный}|\text{создан сильный})
  $$
\end{frame}

\begin{frame}
  \frametitle{Конечный автомат}
  \begin{tikzpicture}[shorten >=1pt,auto,node distance=3.5cm]
  \node[initial,state] (t1)      {сильный};
  \node[state]         (t2a) [above of=t1]  {толчок};
  \node[state]         (t2d) [right of=t2a] {ветер};
  \node[state]         (t2b) [right of=t1] {искусственный};
  \node[state]         (t2c) [below right of=t2b] {интеллект};
  \node[state]         (t2e) [above right of=t2b] {свет}; 
 
  \path[->] (t1)  edge [line width=1pt]  node {0.08} (t2a)
  edge [line width=0.1pt]             node {0.01} (t2b)
  (t2b) edge [bend left, line width=1pt] node {0.11} (t2e)
                  (t2b) edge [bend right, line width=9pt]  node {0.89} (t2c)
                  (t1) edge [bend left, line width=9pt]             node {0.91} (t2d);
\end{tikzpicture}
\end{frame}


\end{document}
